{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DSA 2040 Practical Exam - Section 2: Data Mining\n",
                "\n",
                "**Student Name:** Arnold Bophine Odiyo  \n",
                "**Student ID:** 821  \n",
                "**Date:** December 11, 2025\n",
                "\n",
                "---\n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook demonstrates comprehensive **Data Mining** tasks using the Iris dataset. The workflow includes:\n",
                "\n",
                "### Objectives:\n",
                "1. **Preprocessing** - Load, clean, normalize, and explore the Iris dataset\n",
                "2. **Clustering** - Apply K-Means clustering and evaluate performance\n",
                "3. **Classification** - Build Decision Tree and KNN classifiers\n",
                "4. **Association Rule Mining** - Discover patterns in synthetic market basket data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Required Libraries\n",
                "\n",
                "We'll use:\n",
                "- `pandas` and `numpy` for data manipulation\n",
                "- `sklearn` for machine learning algorithms\n",
                "- `matplotlib` and `seaborn` for visualizations\n",
                "- `mlxtend` for association rule mining"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import random\n",
                "\n",
                "# Sklearn imports\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.metrics import adjusted_rand_score, classification_report, accuracy_score\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "# Association rule mining\n",
                "from mlxtend.preprocessing import TransactionEncoder\n",
                "from mlxtend.frequent_patterns import apriori, association_rules\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.dpi'] = 100\n",
                "\n",
                "print(\"✓ All libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup File Paths\n",
                "\n",
                "Configure paths for saving outputs and visualizations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get current working directory\n",
                "CURRENT_DIR = os.getcwd()\n",
                "\n",
                "# Determine project root\n",
                "if os.path.basename(CURRENT_DIR) == 'DataMining':\n",
                "    BASE_DIR = CURRENT_DIR\n",
                "else:\n",
                "    BASE_DIR = os.path.join(CURRENT_DIR, 'DataMining')\n",
                "\n",
                "# Create output directory if it doesn't exist\n",
                "os.makedirs(BASE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Current Directory: {CURRENT_DIR}\")\n",
                "print(f\"Output Directory: {BASE_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: Data Preprocessing\n",
                "\n",
                "## 1.1 Load and Preprocess Iris Dataset\n",
                "\n",
                "**Goal:** Load the Iris dataset and prepare it for analysis.\n",
                "\n",
                "**Steps:**\n",
                "- Load data from sklearn\n",
                "- Check for missing values\n",
                "- Normalize features using Min-Max scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 1.1 Load and Preprocess Iris Dataset ---\")\n",
                "\n",
                "# 1. Load Data\n",
                "iris = load_iris()\n",
                "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
                "df['target'] = iris.target\n",
                "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
                "\n",
                "print(\"\\nData Sample (First 5 rows):\")\n",
                "display(df.head())\n",
                "\n",
                "print(f\"\\nDataset Shape: {df.shape}\")\n",
                "print(f\"Features: {iris.feature_names}\")\n",
                "print(f\"Classes: {list(iris.target_names)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Handle Missing Values\n",
                "print(\"\\nChecking for missing values:\")\n",
                "print(df.isnull().sum())\n",
                "print(\"✓ No missing values in Iris dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Normalize Features (Min-Max Scaling)\n",
                "print(\"\\nNormalizing features (Min-Max Scaling)...\")\n",
                "scaler = MinMaxScaler()\n",
                "feature_cols = iris.feature_names\n",
                "df_normalized = df.copy()\n",
                "df_normalized[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
                "\n",
                "print(\"Normalized Data Sample:\")\n",
                "display(df_normalized.head())\n",
                "\n",
                "print(\"\\n✓ Preprocessing Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Exploratory Data Analysis (EDA)\n",
                "\n",
                "**Goal:** Understand the data distribution and relationships.\n",
                "\n",
                "**Analysis:**\n",
                "1. Summary statistics\n",
                "2. Pairplot to visualize feature relationships\n",
                "3. Correlation heatmap\n",
                "4. Boxplots for outlier detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 1.3 Exploratory Data Analysis (EDA) ---\")\n",
                "\n",
                "# 1. Summary Statistics\n",
                "print(\"\\nSummary Statistics:\")\n",
                "display(df.describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Pairplot\n",
                "print(\"Generating Pairplot...\")\n",
                "sns.pairplot(df, hue='species', diag_kind='hist', palette='viridis')\n",
                "plt.savefig(os.path.join(BASE_DIR, 'pairplot.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Pairplot saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Correlation Heatmap\n",
                "print(\"Generating Correlation Heatmap...\")\n",
                "plt.figure(figsize=(10, 8))\n",
                "corr = df[feature_cols].corr()\n",
                "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, linewidths=1)\n",
                "plt.title('Correlation Matrix of Iris Features', fontsize=14, fontweight='bold')\n",
                "plt.savefig(os.path.join(BASE_DIR, 'correlation_heatmap.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Correlation heatmap saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Outlier Detection (Boxplots)\n",
                "print(\"Generating Boxplots for Outlier Detection...\")\n",
                "plt.figure(figsize=(12, 6))\n",
                "df_melted = pd.melt(df, id_vars=['species'], value_vars=feature_cols)\n",
                "sns.boxplot(x='variable', y='value', data=df_melted, palette='Set2')\n",
                "plt.title('Boxplot of Iris Features', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Feature', fontsize=12)\n",
                "plt.ylabel('Value', fontsize=12)\n",
                "plt.xticks(rotation=15)\n",
                "plt.savefig(os.path.join(BASE_DIR, 'boxplots.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Boxplots saved\")\n",
                "\n",
                "print(\"\\n✓ EDA Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Train-Test Split\n",
                "\n",
                "**Goal:** Split the normalized data for machine learning tasks.\n",
                "\n",
                "**Configuration:**\n",
                "- Test size: 20%\n",
                "- Random state: 42 (for reproducibility)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 1.4 Train-Test Split ---\")\n",
                "X = df_normalized[feature_cols]\n",
                "y = df_normalized['target']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Train Set Shape: {X_train.shape}\")\n",
                "print(f\"Test Set Shape: {X_test.shape}\")\n",
                "print(f\"\\nClass Distribution in Training Set:\")\n",
                "print(y_train.value_counts().sort_index())\n",
                "\n",
                "print(\"\\n✓ Data Split Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: Data Clustering\n",
                "\n",
                "## 2.1 K-Means Clustering (k=3)\n",
                "\n",
                "**Goal:** Apply K-Means clustering to group similar iris flowers.\n",
                "\n",
                "**Evaluation Metric:** Adjusted Rand Index (ARI) - measures similarity between predicted clusters and true labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 2. Data Clustering ---\")\n",
                "\n",
                "# Prepare data for clustering (using normalized features)\n",
                "X_clustering = df_normalized[feature_cols].values\n",
                "y_true = df_normalized['target'].values\n",
                "\n",
                "# 2.1 K-Means with k=3\n",
                "print(\"\\nRunning K-Means (k=3)...\")\n",
                "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
                "y_pred = kmeans.fit_predict(X_clustering)\n",
                "\n",
                "# Calculate ARI\n",
                "ari = adjusted_rand_score(y_true, y_pred)\n",
                "print(f\"\\n✓ K-Means Clustering Complete\")\n",
                "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
                "print(f\"\\nInterpretation: ARI ranges from -1 to 1\")\n",
                "print(f\"  • 1.0 = Perfect clustering\")\n",
                "print(f\"  • 0.0 = Random clustering\")\n",
                "print(f\"  • {ari:.4f} indicates {'excellent' if ari > 0.8 else 'good' if ari > 0.6 else 'moderate'} agreement with true labels\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Elbow Method - Finding Optimal k\n",
                "\n",
                "**Goal:** Determine the optimal number of clusters.\n",
                "\n",
                "**Method:** Plot inertia (within-cluster sum of squares) for different k values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nGenerating Elbow Curve...\")\n",
                "inertia = []\n",
                "k_values = range(2, 11)\n",
                "\n",
                "for k in k_values:\n",
                "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    km.fit(X_clustering)\n",
                "    inertia.append(km.inertia_)\n",
                "\n",
                "# Plot Elbow Curve\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(k_values, inertia, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
                "plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
                "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(k_values)\n",
                "plt.savefig(os.path.join(BASE_DIR, 'elbow_curve.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Elbow curve saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Cluster Visualization\n",
                "\n",
                "**Goal:** Visualize the clusters in 2D space.\n",
                "\n",
                "**Features Used:** Petal Length vs Petal Width (most discriminative features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nVisualizing Clusters...\")\n",
                "\n",
                "# Create DataFrame for plotting\n",
                "df_plot = pd.DataFrame(X_clustering, columns=feature_cols)\n",
                "df_plot['Cluster'] = y_pred\n",
                "df_plot['True Species'] = df['species'].values\n",
                "\n",
                "# Plot using Petal Length (index 2) vs Petal Width (index 3)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Predicted Clusters\n",
                "sns.scatterplot(\n",
                "    data=df_plot,\n",
                "    x=feature_cols[2],\n",
                "    y=feature_cols[3],\n",
                "    hue='Cluster',\n",
                "    palette='viridis',\n",
                "    s=100,\n",
                "    alpha=0.7,\n",
                "    ax=axes[0]\n",
                ")\n",
                "\n",
                "# Plot Centroids\n",
                "centroids = kmeans.cluster_centers_\n",
                "axes[0].scatter(\n",
                "    centroids[:, 2],\n",
                "    centroids[:, 3],\n",
                "    c='red',\n",
                "    s=300,\n",
                "    marker='X',\n",
                "    edgecolors='black',\n",
                "    linewidths=2,\n",
                "    label='Centroids'\n",
                ")\n",
                "axes[0].set_title('K-Means Clusters (Predicted)', fontsize=12, fontweight='bold')\n",
                "axes[0].legend()\n",
                "\n",
                "# True Species\n",
                "sns.scatterplot(\n",
                "    data=df_plot,\n",
                "    x=feature_cols[2],\n",
                "    y=feature_cols[3],\n",
                "    hue='True Species',\n",
                "    palette='Set2',\n",
                "    s=100,\n",
                "    alpha=0.7,\n",
                "    ax=axes[1]\n",
                ")\n",
                "axes[1].set_title('True Species Labels', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(BASE_DIR, 'clusters_scatter.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Cluster visualization saved\")\n",
                "\n",
                "print(\"\\n✓ Clustering Analysis Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: Classification\n",
                "\n",
                "## 3.1 Decision Tree Classifier\n",
                "\n",
                "**Goal:** Build a Decision Tree to classify iris species.\n",
                "\n",
                "**Advantages:**\n",
                "- Interpretable (can visualize decision rules)\n",
                "- Handles non-linear relationships\n",
                "- No feature scaling required"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 3.1 Classification (Iris) ---\")\n",
                "\n",
                "# Use original (non-normalized) data for Decision Tree\n",
                "X_orig = df[feature_cols]\n",
                "y_orig = df['target']\n",
                "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
                "    X_orig, y_orig, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# 3.1.1 Decision Tree\n",
                "print(\"\\nTraining Decision Tree...\")\n",
                "dt = DecisionTreeClassifier(random_state=42)\n",
                "dt.fit(X_train_orig, y_train_orig)\n",
                "y_pred_dt = dt.predict(X_test_orig)\n",
                "\n",
                "print(\"\\nDecision Tree Metrics:\")\n",
                "print(classification_report(y_test_orig, y_pred_dt, target_names=iris.target_names))\n",
                "\n",
                "acc_dt = accuracy_score(y_test_orig, y_pred_dt)\n",
                "print(f\"Test Accuracy: {acc_dt:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Decision Tree\n",
                "print(\"\\nVisualizing Decision Tree...\")\n",
                "plt.figure(figsize=(20, 12))\n",
                "plot_tree(\n",
                "    dt,\n",
                "    filled=True,\n",
                "    feature_names=feature_cols,\n",
                "    class_names=iris.target_names,\n",
                "    rounded=True,\n",
                "    fontsize=10\n",
                ")\n",
                "plt.title(\"Decision Tree Visualization\", fontsize=16, fontweight='bold')\n",
                "plt.savefig(os.path.join(BASE_DIR, 'decision_tree.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"✓ Decision Tree plot saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 K-Nearest Neighbors (KNN) Classifier\n",
                "\n",
                "**Goal:** Build a KNN classifier for comparison.\n",
                "\n",
                "**Configuration:**\n",
                "- k = 5 neighbors\n",
                "- Uses normalized data (distance-based algorithm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1.2 KNN (using normalized data)\n",
                "print(\"\\nTraining KNN (k=5)...\")\n",
                "knn = KNeighborsClassifier(n_neighbors=5)\n",
                "knn.fit(X_train, y_train)\n",
                "y_pred_knn = knn.predict(X_test)\n",
                "\n",
                "print(\"\\nKNN Metrics:\")\n",
                "print(classification_report(y_test, y_pred_knn, target_names=iris.target_names))\n",
                "\n",
                "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
                "print(f\"Test Accuracy: {acc_knn:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Model Comparison\n",
                "\n",
                "**Goal:** Compare Decision Tree vs KNN performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Scores\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"MODEL COMPARISON\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Decision Tree Test Accuracy: {acc_dt:.4f}\")\n",
                "print(f\"KNN Test Accuracy:           {acc_knn:.4f}\")\n",
                "print(f\"\\nBetter Model: {'Decision Tree' if acc_dt > acc_knn else 'KNN' if acc_knn > acc_dt else 'Tie'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.4 Cross-Validation Analysis\n",
                "\n",
                "**Goal:** Validate model robustness using 5-fold cross-validation.\n",
                "\n",
                "**Why Cross-Validation?**\n",
                "- Single train-test split may give overly optimistic results\n",
                "- Cross-validation provides more reliable performance estimates\n",
                "- Helps detect overfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-Validation (to address 100% \"too good to be true\" concern)\n",
                "print(\"\\n--- Cross-Validation (5-Fold) ---\")\n",
                "\n",
                "# For Decision Tree (on original data)\n",
                "cv_dt = cross_val_score(dt, X_orig, y_orig, cv=5)\n",
                "\n",
                "# For KNN (on normalized data)\n",
                "X_norm_all = df_normalized[feature_cols]\n",
                "y_norm_all = df_normalized['target']\n",
                "cv_knn = cross_val_score(knn, X_norm_all, y_norm_all, cv=5)\n",
                "\n",
                "print(f\"\\nDecision Tree CV Accuracy: {cv_dt.mean():.4f} (+/- {cv_dt.std() * 2:.4f})\")\n",
                "print(f\"  Individual Folds: {[f'{x:.4f}' for x in cv_dt]}\")\n",
                "\n",
                "print(f\"\\nKNN CV Accuracy:           {cv_knn.mean():.4f} (+/- {cv_knn.std() * 2:.4f})\")\n",
                "print(f\"  Individual Folds: {[f'{x:.4f}' for x in cv_knn]}\")\n",
                "\n",
                "print(\"\\n✓ Classification Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: Association Rule Mining\n",
                "\n",
                "## 4.1 Generate Synthetic Market Basket Data\n",
                "\n",
                "**Goal:** Create synthetic transaction data for association rule mining.\n",
                "\n",
                "**Data Generation:**\n",
                "- 50 transactions\n",
                "- 10 different items\n",
                "- Basket size: 3-6 items\n",
                "- Injected pattern: Bread → Butter (70% co-occurrence)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- 3.2 Association Rule Mining (Market Basket) ---\")\n",
                "\n",
                "# 3.2.1 Generate Synthetic Transactions\n",
                "print(\"Generating Synthetic Transaction Data...\")\n",
                "items = ['Milk', 'Bread', 'Butter', 'Eggs', 'Cheese', 'Yogurt', 'Apple', 'Banana', 'Coffee', 'Tea']\n",
                "\n",
                "# Seed for reproducibility\n",
                "random.seed(42)\n",
                "\n",
                "transactions = []\n",
                "num_transactions = 50\n",
                "\n",
                "for _ in range(num_transactions):\n",
                "    # Create random basket size 3-6 items\n",
                "    basket_size = random.randint(3, 6)\n",
                "    basket = random.sample(items, basket_size)\n",
                "    \n",
                "    # Inject patterns (e.g., Bread + Butter often together)\n",
                "    if 'Bread' in basket and 'Butter' not in basket:\n",
                "        if random.random() > 0.3:  # 70% chance to add butter if bread is there\n",
                "            basket[-1] = 'Butter'  # Replace last item\n",
                "    \n",
                "    transactions.append(basket)\n",
                "\n",
                "print(f\"\\n✓ Generated {len(transactions)} transactions\")\n",
                "print(f\"\\nSample Transactions:\")\n",
                "for i in range(5):\n",
                "    print(f\"  Transaction {i+1}: {transactions[i]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 Apriori Algorithm\n",
                "\n",
                "**Goal:** Find frequent itemsets using the Apriori algorithm.\n",
                "\n",
                "**Configuration:**\n",
                "- Minimum support: 0.2 (20%)\n",
                "- An itemset must appear in at least 10 transactions to be considered frequent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess for Apriori (One-Hot Encoding)\n",
                "print(\"\\nPreprocessing transactions...\")\n",
                "te = TransactionEncoder()\n",
                "te_ary = te.fit(transactions).transform(transactions)\n",
                "df_trans = pd.DataFrame(te_ary, columns=te.columns_)\n",
                "\n",
                "print(f\"Transaction Matrix Shape: {df_trans.shape}\")\n",
                "print(\"\\nTransaction Matrix Sample:\")\n",
                "display(df_trans.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2.2 Apriori\n",
                "print(\"\\nRunning Apriori (Min Support=0.2)...\")\n",
                "frequent_itemsets = apriori(df_trans, min_support=0.2, use_colnames=True)\n",
                "\n",
                "print(f\"\\n✓ Found {len(frequent_itemsets)} frequent itemsets\")\n",
                "print(\"\\nTop 10 Frequent Itemsets:\")\n",
                "display(frequent_itemsets.sort_values('support', ascending=False).head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.3 Association Rules\n",
                "\n",
                "**Goal:** Extract association rules from frequent itemsets.\n",
                "\n",
                "**Metrics:**\n",
                "- **Support:** How often the itemset appears\n",
                "- **Confidence:** How often the rule is true\n",
                "- **Lift:** How much more likely the consequent is given the antecedent\n",
                "  - Lift > 1: Positive correlation\n",
                "  - Lift = 1: Independent\n",
                "  - Lift < 1: Negative correlation\n",
                "\n",
                "**Configuration:**\n",
                "- Minimum confidence: 0.5 (50%)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2.3 Association Rules\n",
                "print(\"\\nExtracting Rules (Min Confidence=0.5)...\")\n",
                "\n",
                "if not frequent_itemsets.empty:\n",
                "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
                "    \n",
                "    # Sort by Lift\n",
                "    rules = rules.sort_values('lift', ascending=False)\n",
                "    \n",
                "    print(f\"\\n✓ Found {len(rules)} association rules\")\n",
                "    print(\"\\nTop 10 Association Rules (sorted by Lift):\")\n",
                "    display(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
                "    \n",
                "    # Save rules\n",
                "    rules.to_csv(os.path.join(BASE_DIR, 'association_rules.csv'), index=False)\n",
                "    print(f\"\\n✓ Rules saved to association_rules.csv\")\n",
                "    \n",
                "    # Insights\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"KEY INSIGHTS\")\n",
                "    print(\"=\"*60)\n",
                "    if len(rules) > 0:\n",
                "        top_rule = rules.iloc[0]\n",
                "        print(f\"\\nStrongest Rule (Highest Lift):\")\n",
                "        print(f\"  {set(top_rule['antecedents'])} → {set(top_rule['consequents'])}\")\n",
                "        print(f\"  Support: {top_rule['support']:.3f}\")\n",
                "        print(f\"  Confidence: {top_rule['confidence']:.3f}\")\n",
                "        print(f\"  Lift: {top_rule['lift']:.3f}\")\n",
                "        print(f\"\\n  Interpretation: Customers who buy {set(top_rule['antecedents'])}\")\n",
                "        print(f\"  are {top_rule['lift']:.2f}x more likely to buy {set(top_rule['consequents'])}\")\n",
                "else:\n",
                "    print(\"No frequent itemsets found with current support threshold.\")\n",
                "\n",
                "print(\"\\n✓ Association Rule Mining Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Summary and Conclusions\n",
                "\n",
                "## Task Completion Summary\n",
                "\n",
                "### ✓ Part 1: Data Preprocessing\n",
                "- Loaded and normalized Iris dataset (150 samples, 4 features)\n",
                "- No missing values detected\n",
                "- Generated comprehensive EDA visualizations\n",
                "- Split data: 120 training, 30 testing samples\n",
                "\n",
                "### ✓ Part 2: Clustering\n",
                "- Applied K-Means clustering (k=3)\n",
                "- Achieved high ARI score indicating good cluster quality\n",
                "- Elbow method confirmed k=3 as optimal\n",
                "- Visualized clusters vs true species labels\n",
                "\n",
                "### ✓ Part 3: Classification\n",
                "- **Decision Tree:** Interpretable model with visualization\n",
                "- **KNN (k=5):** Distance-based classifier\n",
                "- Both models achieved excellent test accuracy\n",
                "- Cross-validation confirmed robust performance\n",
                "\n",
                "### ✓ Part 4: Association Rule Mining\n",
                "- Generated 50 synthetic market basket transactions\n",
                "- Discovered frequent itemsets using Apriori\n",
                "- Extracted association rules with confidence ≥ 50%\n",
                "- Identified strong product associations (e.g., Bread → Butter)\n",
                "\n",
                "## Key Insights\n",
                "\n",
                "1. **Iris Dataset Characteristics:**\n",
                "   - Petal features (length & width) are most discriminative\n",
                "   - Strong correlation between petal length and width\n",
                "   - Setosa is easily separable; Versicolor and Virginica overlap slightly\n",
                "\n",
                "2. **Model Performance:**\n",
                "   - Both classifiers performed excellently on this well-separated dataset\n",
                "   - Cross-validation validates that results are not due to lucky train-test split\n",
                "   - Decision Tree offers interpretability advantage\n",
                "\n",
                "3. **Association Rules:**\n",
                "   - Successfully identified injected pattern (Bread → Butter)\n",
                "   - Lift metric helps prioritize actionable rules for business decisions\n",
                "   - Can be used for product placement and recommendation systems\n",
                "\n",
                "---\n",
                "\n",
                "**All Data Mining tasks completed successfully!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}