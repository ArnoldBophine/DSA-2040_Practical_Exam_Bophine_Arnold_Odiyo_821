{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA 2040 Practical Exam - Section 1: Data Warehousing\n",
    "\n",
    "**Student Name:** Arnold Bophine Odiyo  \n",
    "**Student ID:** 821  \n",
    "**Date:** December 11, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the complete **ETL (Extract, Transform, Load)** pipeline for building a Data Warehouse using a **Star Schema** design. The warehouse is designed to support OLAP queries for retail sales analysis.\n",
    "\n",
    "### Objectives:\n",
    "1. **Extract** data from the Online Retail CSV dataset\n",
    "2. **Transform** the data (clean, calculate metrics, simulate current dates)\n",
    "3. **Load** data into an SQLite Data Warehouse\n",
    "4. **Visualize** insights from the warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use:\n",
    "- `pandas` for data manipulation\n",
    "- `sqlite3` for database operations\n",
    "- `matplotlib` and `seaborn` for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Determine project root\n",
    "# If we're in DataWarehousing folder, go up one level\n",
    "if os.path.basename(CURRENT_DIR) == 'DataWarehousing':\n",
    "    BASE_DIR = os.path.dirname(CURRENT_DIR)\n",
    "else:\n",
    "    BASE_DIR = CURRENT_DIR\n",
    "\n",
    "# Define file paths\n",
    "DATA_FILE = os.path.join(BASE_DIR, 'Copy of Online Retail.csv')\n",
    "DB_FILE = os.path.join(BASE_DIR, 'DataWarehousing', 'retail_dw.db')\n",
    "SCHEMA_FILE = os.path.join(BASE_DIR, 'DataWarehousing', 'warehouse_schema.sql')\n",
    "\n",
    "print(f\"Current Directory: {CURRENT_DIR}\")\n",
    "print(f\"Project Root: {BASE_DIR}\")\n",
    "print(f\"\\nFile Paths:\")\n",
    "print(f\"  Data CSV: {DATA_FILE}\")\n",
    "print(f\"  Database: {DB_FILE}\")\n",
    "print(f\"  Schema: {SCHEMA_FILE}\")\n",
    "print(f\"\\nFile Exists:\")\n",
    "print(f\"  ✓ Data CSV: {os.path.exists(DATA_FILE)}\")\n",
    "print(f\"  ✓ Schema SQL: {os.path.exists(SCHEMA_FILE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def init_db():\n",
    "    \"\"\"Initialize the database with schema.\"\"\"\n",
    "    print(\"Initializing Database...\")\n",
    "    \n",
    "    # Close any existing connections first\n",
    "    import gc\n",
    "    gc.collect()  # Force garbage collection to close any lingering connections\n",
    "    \n",
    "    if os.path.exists(DB_FILE):\n",
    "        try:\n",
    "            os.remove(DB_FILE)\n",
    "            print(\"Removed existing database file.\")\n",
    "        except PermissionError:\n",
    "            print(\"⚠ Database file is in use. Skipping deletion (will use existing database).\")\n",
    "            print(\"  If you need a fresh database, restart the kernel and run again.\")\n",
    "            return\n",
    "    \n",
    "    try:\n",
    "        with open(SCHEMA_FILE, 'r') as f:\n",
    "            schema_sql = f.read()\n",
    "        \n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.executescript(schema_sql)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(\"✓ Database initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error initializing DB: {e}\")\n",
    "\n",
    "# Execute initialization\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Determine project root\n",
    "# If we're in DataWarehousing folder, go up one level\n",
    "if os.path.basename(CURRENT_DIR) == 'DataWarehousing':\n",
    "    BASE_DIR = os.path.dirname(CURRENT_DIR)\n",
    "else:\n",
    "    BASE_DIR = CURRENT_DIR\n",
    "\n",
    "# Define file paths\n",
    "DATA_FILE = os.path.join(BASE_DIR, 'Copy of Online Retail.csv')\n",
    "DB_FILE = os.path.join(BASE_DIR, 'DataWarehousing', 'retail_dw.db')\n",
    "SCHEMA_FILE = os.path.join(BASE_DIR, 'DataWarehousing', 'warehouse_schema.sql')\n",
    "\n",
    "print(f\"Current Directory: {CURRENT_DIR}\")\n",
    "print(f\"Project Root: {BASE_DIR}\")\n",
    "print(f\"\\nFile Paths:\")\n",
    "print(f\"  Data CSV: {DATA_FILE}\")\n",
    "print(f\"  Database: {DB_FILE}\")\n",
    "print(f\"  Schema: {SCHEMA_FILE}\")\n",
    "print(f\"\\nFile Exists:\")\n",
    "print(f\"  ✓ Data CSV: {os.path.exists(DATA_FILE)}\")\n",
    "print(f\"  ✓ Schema SQL: {os.path.exists(SCHEMA_FILE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Database\n",
    "\n",
    "This function:\n",
    "- Removes any existing database (for fresh start)\n",
    "- Reads the Star Schema SQL definition\n",
    "- Creates tables: `CustomerDim`, `ProductDim`, `TimeDim`, and `SalesFact`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    \"\"\"Initialize the database with schema.\"\"\"\n",
    "    print(\"Initializing Database...\")\n",
    "    if os.path.exists(DB_FILE):\n",
    "        os.remove(DB_FILE)\n",
    "        print(\"Removed existing database file.\")\n",
    "    try:\n",
    "        with open(SCHEMA_FILE, 'r') as f:\n",
    "            schema_sql = f.read()\n",
    "        \n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.executescript(schema_sql)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(\"✓ Database initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error initializing DB: {e}\")\n",
    "\n",
    "# Execute initialization\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXTRACT Phase\n",
    "\n",
    "**Goal:** Load raw data from the CSV file.\n",
    "\n",
    "**Challenges:**\n",
    "- The dataset may have encoding issues (common in legacy systems)\n",
    "- We handle this by trying multiple encodings\n",
    "\n",
    "**Expected Output:** ~541,000 raw transaction records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_path):\n",
    "    \"\"\"Task 2.2: Extract Phase\"\"\"\n",
    "    print(f\"\\n--- EXTRACT PHASE ---\")\n",
    "    print(f\"Reading data from: {file_path}\")\n",
    "    try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            \n",
    "        print(f\"✓ Extracted {len(df):,} rows.\")\n",
    "        print(f\"\\nColumns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute extraction\n",
    "df_raw = extract_data(DATA_FILE)\n",
    "\n",
    "# Display sample\n",
    "if df_raw is not None:\n",
    "    print(\"\\nSample Data:\")\n",
    "    display(df_raw.head())\n",
    "    print(f\"\\nData Types:\\n{df_raw.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TRANSFORM Phase\n",
    "\n",
    "This is the most complex phase with multiple operations:\n",
    "\n",
    "### 5.1 Data Cleaning\n",
    "- Remove rows with missing `CustomerID` (can't link to customer dimension)\n",
    "- Filter out invalid transactions (negative quantities, zero prices)\n",
    "\n",
    "### 5.2 Feature Engineering\n",
    "- Calculate `TotalSales = Quantity × UnitPrice`\n",
    "\n",
    "### 5.3 Date Simulation\n",
    "- Original data is from 2010-2011\n",
    "- Shift all dates to 2024-2025 to simulate current data\n",
    "- Filter to last 12 months (Aug 2024 - Aug 2025)\n",
    "\n",
    "### 5.4 Dimension Extraction\n",
    "- Extract unique customers → `CustomerDim`\n",
    "- Extract unique products → `ProductDim`\n",
    "- Extract unique dates with time attributes → `TimeDim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"Task 2.3: Transform Phase\"\"\"\n",
    "    print(f\"\\n--- TRANSFORM PHASE ---\")\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # 1. Drop missing CustomerID\n",
    "    print(\"\\n[1/6] Dropping rows with missing CustomerID...\")\n",
    "    df = df.dropna(subset=['CustomerID']).copy()\n",
    "    print(f\"  Rows after cleaning: {len(df):,} (Dropped {initial_count - len(df):,})\")\n",
    "    \n",
    "    # 2. Convert types\n",
    "    print(\"\\n[2/6] Converting data types...\")\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    df['CustomerID'] = df['CustomerID'].astype(int)\n",
    "    print(\"  ✓ Dates and IDs converted\")\n",
    "    \n",
    "    # 3. Calculate TotalSales\n",
    "    print(\"\\n[3/6] Calculating TotalSales...\")\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    print(f\"  ✓ TotalSales calculated (Sample: ${df['TotalSales'].iloc[0]:.2f})\")\n",
    "    \n",
    "    # 4. Simulate 2025 Data (Date Shifting)\n",
    "    print(\"\\n[4/6] Simulating 2024-2025 Data: Shifting dates...\")\n",
    "    max_date = df['InvoiceDate'].max()\n",
    "    target_date = pd.Timestamp('2025-08-12')\n",
    "    time_delta = target_date - max_date\n",
    "    \n",
    "    df['InvoiceDate'] = df['InvoiceDate'] + time_delta\n",
    "    print(f\"  Original max date: {max_date.date()}\")\n",
    "    print(f\"  New date range: {df['InvoiceDate'].min().date()} to {df['InvoiceDate'].max().date()}\")\n",
    "    \n",
    "    # Filter for last year\n",
    "    start_date = target_date - pd.DateOffset(years=1)\n",
    "    df = df[(df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] <= target_date)]\n",
    "    \n",
    "    # Remove invalid transactions\n",
    "    print(\"\\n[5/6] Filtering invalid quantities and prices...\")\n",
    "    df = df[df['Quantity'] > 0]\n",
    "    df = df[df['UnitPrice'] > 0]\n",
    "    print(f\"  ✓ Rows after filtering: {len(df):,}\")\n",
    "    \n",
    "    # 5. Extract Dimensions\n",
    "    print(\"\\n[6/6] Extracting Dimensions...\")\n",
    "    \n",
    "    # Customer Dimension\n",
    "    customer_dim = df.groupby('CustomerID').agg({\n",
    "        'Country': 'first'\n",
    "    }).reset_index()\n",
    "    customer_dim.columns = ['customer_id', 'country']\n",
    "    customer_dim['source_customer_id'] = customer_dim['customer_id'].astype(str)\n",
    "    customer_dim['name'] = 'Customer ' + customer_dim['source_customer_id']\n",
    "    print(f\"  ✓ CustomerDim: {len(customer_dim):,} unique customers\")\n",
    "    \n",
    "    # Time Dimension\n",
    "    unique_dates = df['InvoiceDate'].dt.date.unique()\n",
    "    time_dim = pd.DataFrame({'full_date': unique_dates})\n",
    "    time_dim['full_date'] = pd.to_datetime(time_dim['full_date'])\n",
    "    time_dim['time_id'] = time_dim['full_date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    time_dim['day'] = time_dim['full_date'].dt.day\n",
    "    time_dim['month'] = time_dim['full_date'].dt.month\n",
    "    time_dim['year'] = time_dim['full_date'].dt.year\n",
    "    time_dim['quarter'] = time_dim['full_date'].dt.quarter\n",
    "    time_dim['day_of_week'] = time_dim['full_date'].dt.day_name()\n",
    "    print(f\"  ✓ TimeDim: {len(time_dim):,} unique dates\")\n",
    "    \n",
    "    # Product Dimension\n",
    "    product_dim = df.groupby('StockCode').agg({\n",
    "        'Description': 'first'\n",
    "    }).reset_index()\n",
    "    product_dim.columns = ['stock_code', 'description']\n",
    "    product_dim['product_id'] = product_dim.index + 1\n",
    "    product_dim['category'] = 'General'\n",
    "    print(f\"  ✓ ProductDim: {len(product_dim):,} unique products\")\n",
    "    \n",
    "    # Prepare Sales Fact\n",
    "    print(\"\\nPreparing Sales Fact Table...\")\n",
    "    fact_table = df.merge(product_dim[['stock_code', 'product_id']], \n",
    "                          left_on='StockCode', right_on='stock_code', how='left')\n",
    "    fact_table['time_id'] = fact_table['InvoiceDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    sales_fact = fact_table[[\n",
    "        'CustomerID', 'product_id', 'time_id', 'InvoiceNo', 'Quantity', 'UnitPrice', 'TotalSales'\n",
    "    ]].copy()\n",
    "    sales_fact.columns = ['customer_id', 'product_id', 'time_id', 'invoice_no', \n",
    "                          'quantity', 'unit_price', 'total_sales']\n",
    "    print(f\"  ✓ SalesFact: {len(sales_fact):,} transactions\")\n",
    "    \n",
    "    return {\n",
    "        'CustomerDim': customer_dim,\n",
    "        'ProductDim': product_dim,\n",
    "        'TimeDim': time_dim,\n",
    "        'SalesFact': sales_fact\n",
    "    }\n",
    "\n",
    "# Execute transformation\n",
    "if df_raw is not None:\n",
    "    data_staging = transform_data(df_raw)\n",
    "    \n",
    "    # Display samples from each dimension\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSFORMATION COMPLETE - Sample Data:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nCustomerDim Sample:\")\n",
    "    display(data_staging['CustomerDim'].head())\n",
    "    \n",
    "    print(\"\\nProductDim Sample:\")\n",
    "    display(data_staging['ProductDim'].head())\n",
    "    \n",
    "    print(\"\\nTimeDim Sample:\")\n",
    "    display(data_staging['TimeDim'].head())\n",
    "    \n",
    "    print(\"\\nSalesFact Sample:\")\n",
    "    display(data_staging['SalesFact'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LOAD Phase\n",
    "\n",
    "**Goal:** Persist the transformed data into the SQLite Data Warehouse.\n",
    "\n",
    "**Process:**\n",
    "1. Load dimension tables first (CustomerDim, ProductDim, TimeDim)\n",
    "2. Load fact table (SalesFact) with foreign key references\n",
    "\n",
    "**Note:** Using `if_exists='append'` to add data without replacing existing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dict):\n",
    "    \"\"\"Task 2.4: Load Phase\"\"\"\n",
    "    print(f\"\\n--- LOAD PHASE ---\")\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    \n",
    "    try:\n",
    "        # Load Dimensions\n",
    "        print(\"Loading CustomerDim...\")\n",
    "        data_dict['CustomerDim'].to_sql('CustomerDim', conn, if_exists='append', index=False)\n",
    "        print(\"  ✓ CustomerDim loaded\")\n",
    "        \n",
    "        print(\"Loading ProductDim...\")\n",
    "        data_dict['ProductDim'].to_sql('ProductDim', conn, if_exists='append', index=False)\n",
    "        print(\"  ✓ ProductDim loaded\")\n",
    "        \n",
    "        print(\"Loading TimeDim...\")\n",
    "        data_dict['TimeDim'].to_sql('TimeDim', conn, if_exists='append', index=False)\n",
    "        print(\"  ✓ TimeDim loaded\")\n",
    "        \n",
    "        # Load Fact\n",
    "        print(\"Loading SalesFact...\")\n",
    "        data_dict['SalesFact'].to_sql('SalesFact', conn, if_exists='append', index=False)\n",
    "        print(\"  ✓ SalesFact loaded\")\n",
    "        \n",
    "        print(\"\\n✓ Data Loading Complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Execute loading\n",
    "if 'data_staging' in locals():\n",
    "    load_data(data_staging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Data Warehouse\n",
    "\n",
    "Let's query the database to verify the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table counts\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "\n",
    "print(\"Data Warehouse Table Counts:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "tables = ['CustomerDim', 'ProductDim', 'TimeDim', 'SalesFact']\n",
    "for table in tables:\n",
    "    count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn)['count'][0]\n",
    "    print(f\"{table:15s}: {count:,} records\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. OLAP Query Example\n",
    "\n",
    "Demonstrate a **Roll-up** query: Total Sales by Country and Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll-up Query: Sales by Country and Quarter\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.country,\n",
    "    t.year,\n",
    "    t.quarter,\n",
    "    COUNT(f.sale_id) as num_transactions,\n",
    "    SUM(f.total_sales) as total_sales\n",
    "FROM SalesFact f\n",
    "JOIN CustomerDim c ON f.customer_id = c.customer_id\n",
    "JOIN TimeDim t ON f.time_id = t.time_id\n",
    "GROUP BY c.country, t.year, t.quarter\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "df_olap = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nTop 10 Country-Quarter Combinations by Sales:\")\n",
    "display(df_olap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization: Top 10 Countries by Total Sales\n",
    "\n",
    "Generate a bar chart showing which countries contribute the most revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data():\n",
    "    \"\"\"Task 3.2: Visualize Results\"\"\"\n",
    "    print(f\"\\n--- VISUALIZATION PHASE ---\")\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    \n",
    "    # Query: Total Sales by Country (Top 10)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.country,\n",
    "        SUM(f.total_sales) as total_sales\n",
    "    FROM SalesFact f\n",
    "    JOIN CustomerDim c ON f.customer_id = c.customer_id\n",
    "    GROUP BY c.country\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_viz = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=df_viz, x='total_sales', y='country', hue='country', palette='viridis', legend=False)\n",
    "        plt.title('Top 10 Countries by Total Sales (2024-2025)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Total Sales ($)', fontsize=12)\n",
    "        plt.ylabel('Country', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = os.path.join(BASE_DIR, 'sales_by_country.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Visualization saved to: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error visualizing data: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Generate visualization\n",
    "visualize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Insights\n",
    "\n",
    "### ETL Pipeline Results:\n",
    "- **Extracted:** 541,909 raw records\n",
    "- **Cleaned:** Removed 135,080 records with missing CustomerID\n",
    "- **Loaded:** 384,529 valid transactions into the Data Warehouse\n",
    "\n",
    "### Key Insights:\n",
    "1. **Geographic Concentration:** The United Kingdom dominates sales, indicating heavy domestic focus\n",
    "2. **Data Quality:** ~25% of raw data had missing customer information, highlighting the importance of data cleaning\n",
    "3. **Temporal Simulation:** Successfully shifted dates from 2010-2011 to 2024-2025 for realistic analysis\n",
    "\n",
    "### Star Schema Benefits:\n",
    "- Fast query performance (simple joins)\n",
    "- Easy to understand for business analysts\n",
    "- Supports complex OLAP operations (Roll-up, Drill-down, Slice, Dice)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Use this Data Warehouse for advanced analytics, reporting dashboards, and business intelligence applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
